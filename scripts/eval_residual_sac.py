import os
import sys
import gymnasium as gym
import numpy as np
import torch
from stable_baselines3 import SAC
from tqdm import tqdm

# =========================
# è·¯å¾„ Hack
# =========================
current_file_path = os.path.abspath(__file__)
script_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(script_dir)
source_root = os.path.join(project_root, 'diffusion_policy')
if project_root not in sys.path:
    sys.path.insert(0, project_root)
if source_root not in sys.path:
    sys.path.insert(0, source_root)

# å¼•å…¥ä½ çš„ç¯å¢ƒ
from scripts.residual_env import ResidualPegEnv 

def main():
    # ================= é…ç½®åŒºåŸŸ =================
    # 1. å¡«å…¥ä½  Base Policy çš„æƒé‡è·¯å¾„
    BASE_CKPT = "diffusion_policy/data/outputs/2026.01.26/16.00.38_train_diffusion_unet_hybrid_peg_in_hole/checkpoints/base_policy.ckpt" 
    
    # 2. å¡«å…¥ä½ åˆšåˆšè®­ç»ƒå¥½çš„ SAC æ¨¡å‹è·¯å¾„ (best_model.zip æˆ– latest)
    # é€šå¸¸åœ¨ tensorboard_logs/ä½ çš„å®éªŒå/model_checkpoints/ æˆ–è€…æ˜¯ final_model.zip
    RESIDUAL_MODEL_PATH = "data/models/SAC_Residual_v1/rl_model_105000_steps.zip" 

    CHUNK_SIZE = 4
    MAX_STEPS = 200
    N_EPISODES = 50  # æµ‹ 50 æ¬¡çœ‹çœ‹å®åŠ›
    # ===========================================

    print(f"ğŸ§Š Loading Environment with Base Policy...")
    # æ³¨æ„ï¼šæµ‹è¯•æ—¶ residual_scale è¦å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼(0.01)
    env = ResidualPegEnv(
        base_ckpt_path=BASE_CKPT,
        residual_scale=0.01,    # ğŸ‘ˆ å¿…é¡»æ˜¯ 0.01
        residual_clip=0.2,
        max_steps=MAX_STEPS,
        action_chunk_size=CHUNK_SIZE,
        device="cuda:0"
    )

    print(f"ğŸ”¥ Loading Residual Policy (SAC) from: {RESIDUAL_MODEL_PATH}")
    model = SAC.load(RESIDUAL_MODEL_PATH)

    success_count = 0
    pbar = tqdm(range(N_EPISODES))

    for i in pbar:
        obs, _ = env.reset()
        done = False
        truncated = False
        
        while not (done or truncated):
            # ğŸŸ¢ å…³é”®ï¼šdeterministic=True (å…³æ‰ hand shaking)
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)

            if info.get('is_success', False):
                success_count += 1
                done = True # æå‰ç»“æŸ

        current_sr = success_count / (i + 1)
        pbar.set_postfix({"Success Rate": f"{current_sr:.1%}"})

    print("\n" + "="*50)
    print(f"ğŸ“Š æœ€ç»ˆæˆç»© (Chunk={CHUNK_SIZE}, Scale=0.01)")
    print(f"âœ… Success Rate: {success_count/N_EPISODES:.2%} ({success_count}/{N_EPISODES})")
    print("="*50)

if __name__ == "__main__":
    main()