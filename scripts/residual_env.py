import os
import sys
import gymnasium as gym
import numpy as np
import torch
import collections
from gymnasium import spaces
from omegaconf import OmegaConf
import hydra

# =========================
# Path Hack
# =========================
current_file_path = os.path.abspath(__file__)
script_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(script_dir)
source_root = os.path.join(project_root, 'diffusion_policy')
if project_root not in sys.path:
    sys.path.insert(0, project_root)
if source_root not in sys.path:
    sys.path.insert(0, source_root)

import custom_envs


class ResidualPegEnv(gym.Env):
    """
    Residual RL Environment for Peg-in-Hole (Minimalist Pass-through Version)
    
    Logic:
    - Assume 'FrankaPegInHole-v0' ALREADY returns pre-processed images:
      (3, 96, 96), float32, range [0, 1].
    - We just pass them to the Base Policy directly, exactly like eval_policy_batch.py.
    """

    metadata = {"render_modes": ["rgb_array"]}

    def __init__(
        self,
        base_ckpt_path,
        residual_scale=0.05,
        residual_clip=0.2,
        max_steps=400,  # ä¿æŒ 400ï¼Œä¸ Runner ä¸€è‡´
        device="cuda:0",
        action_chunk_size=1
    ):
        super().__init__()

        self.device = torch.device(device)
        self.residual_scale = residual_scale
        self.residual_clip = residual_clip
        self.max_steps = max_steps
        self.current_step = 0
        self.action_chunk_size = action_chunk_size

        # =========================
        # Underlying Environment
        # =========================
        self.env = gym.make(
            "FrankaPegInHole-v0",
            render_mode="rgb_array",
            control_mode="ee",
            disable_env_checker=True,
            # æ³¨æ„ï¼šå¦‚æœä½ çš„ custom_envs é‡Œé»˜è®¤ max_episode_steps å°±æ˜¯ 400ï¼Œè¿™é‡Œæ”¹ä¸æ”¹éƒ½è¡Œ
            max_episode_steps=max_steps, 
        )

        # ğŸŸ¢ å¼ºåˆ¶ Residual Policy åªè¾“å‡º 3 ç»´åŠ¨ä½œ
        self.residual_action_dim = 3
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.residual_action_dim,),
            dtype=np.float32,
        )

        # =========================
        # Residual Observation
        # =========================
        self.residual_obs_dim = 3
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.residual_obs_dim,),
            dtype=np.float32,
        )

        print("ğŸ§Š Loading Base Policy from:", base_ckpt_path)
        print("ğŸ”§ Config: Scale={}, Chunk={}, Pass-through Mode".format(
            residual_scale, action_chunk_size))

        # =========================
        # Load Base Policy
        # =========================
        self.base_policy = self._load_policy(base_ckpt_path)
        self.base_policy.eval()
        self.base_policy.to(self.device)

        for p in self.base_policy.parameters():
            p.requires_grad = False

        # =========================
        # Buffers
        # =========================
        self.n_obs_steps = 2
        self.obs_deque = collections.deque(maxlen=self.n_obs_steps)
        self.base_action_queue = collections.deque(maxlen=self.action_chunk_size)

    # -------------------------------------------------
    # Base Policy Loader
    # -------------------------------------------------
    def _load_policy(self, ckpt_path):
        run_dir = os.path.dirname(os.path.dirname(ckpt_path))
        cfg_path = os.path.join(run_dir, ".hydra", "config.yaml")
        if not os.path.exists(cfg_path):
            raise FileNotFoundError(f"Config not found at {cfg_path}")

        cfg = OmegaConf.load(cfg_path)
        try:
            cls = hydra.utils.get_class(cfg._target_)
            workspace = cls(cfg)
        except Exception:
            from diffusion_policy.workspace.base_workspace import BaseWorkspace
            workspace = BaseWorkspace(cfg)

        workspace.load_checkpoint(ckpt_path)
        return workspace.model

    # -------------------------------------------------
    # Helper
    # -------------------------------------------------
    def _get_residual_obs(self, obs):
        achieved = obs["achieved_goal"]
        desired = obs["desired_goal"]
        pos_err = achieved - desired
        return pos_err.astype(np.float32)

    # -------------------------------------------------
    # Reset
    # -------------------------------------------------
    def reset(self, seed=None, options=None):
        obs, info = self.env.reset(seed=seed, options=options)

        self.base_policy.reset()
        self.obs_deque.clear()
        self.base_action_queue.clear()

        for _ in range(self.n_obs_steps):
            self.obs_deque.append(obs)

        self.current_step = 0
        return self._get_residual_obs(obs), info

    # -------------------------------------------------
    # Step
    # -------------------------------------------------
    def step(self, residual_action):
        # 1. è·å– Base åŠ¨ä½œ
        base_action = self._get_next_base_action()

        # =========================================================
        # ğŸ›¡ï¸ Z-Axis Gating (é˜²æ’å¢™)
        # =========================================================
        current_obs = self.obs_deque[-1]
        current_pos_err = self._get_residual_obs(current_obs) 
        xy_err = np.linalg.norm(current_pos_err[:2])
        
        # 1cm ä¿æŠ¤é˜ˆå€¼
        if xy_err > 0.01:
            if residual_action[2] < 0:
                residual_action[2] = 0.0
        # =========================================================

        # 2. åŠ¨ä½œåˆæˆ
        residual_action = np.clip(
            residual_action, -self.residual_clip, self.residual_clip
        )
        scaled_residual = residual_action * self.residual_scale

        final_action = base_action.copy()
        final_action[:3] += scaled_residual
        final_action = np.clip(final_action, -1.0, 1.0)

        # 3. ç¯å¢ƒæ‰§è¡Œ
        obs, _, terminated, truncated, info = self.env.step(final_action)

        # =========================================================
        # Reward Calculation
        # =========================================================
        achieved = obs["achieved_goal"]
        desired = obs["desired_goal"]
        dist = np.linalg.norm(achieved - desired)

        # 1. è·ç¦»å¥–åŠ± (ä¿æŒä¸å˜)
        r_dist = 1.0 - np.tanh(10.0 * dist) - 1.0
        
        # 2. æˆåŠŸå¥–åŠ± (ä¿æŒä¸å˜)
        r_success = 0.0
        if info.get("is_success", False):
            progress = 1.0 - self.current_step / self.max_steps
            r_success = 100.0 * progress

        # ğŸŸ¢ 3. [æ–°å¢] åŠ¨ä½œå¹…åº¦æƒ©ç½š (Action Regularization)
        # ç›®çš„ï¼šè®© SAC å­¦ä¼š"éå¿…è¦ä¸ä¹±åŠ¨"ã€‚
        # ä½¿ç”¨ raw residual_action (èŒƒå›´é€šå¸¸æ˜¯ -1 åˆ° 1)ï¼Œè€Œä¸æ˜¯ scale åçš„ã€‚
        # ç³»æ•° 0.05 æ˜¯ç»éªŒå€¼ï¼Œé…åˆ scale=0.01 ä½¿ç”¨æ•ˆæœå¾ˆå¥½ã€‚
        action_norm = np.linalg.norm(residual_action)
        r_penalty = -0.05 * (action_norm ** 2)

        # 4. æ€»å¥–åŠ±
        reward = r_dist + r_success + r_penalty

        # ğŸŸ¢ [å»ºè®®] æŠŠåˆ†é¡¹å¥–åŠ±æ”¾è¿› info é‡Œï¼Œæ–¹ä¾¿åœ¨ TensorBoard è§‚å¯Ÿ SAC æ˜¯å¦åœ¨"å·æ‡’"
        info["r_dist"] = r_dist
        info["r_success"] = r_success
        info["r_penalty"] = r_penalty
        # =========================================================

        self.obs_deque.append(obs)
        self.current_step += 1

        if self.current_step >= self.max_steps:
            truncated = True

        return self._get_residual_obs(obs), reward, terminated, truncated, info

    # -------------------------------------------------
    # Base Policy Inference (æç®€é€ä¼ ç‰ˆ)
    # ğŸŸ¢ å®Œå…¨å¤åˆ» eval_policy_batch.py çš„æ•°æ®æµ
    # -------------------------------------------------
    def _get_next_base_action(self):
        if len(self.base_action_queue) > 0:
            return self.base_action_queue.popleft()

        batch = {"image": [], "image_wrist": [], "state": []}
        
        for o in self.obs_deque:
            batch["image"].append(o["image"])
            batch["image_wrist"].append(o["image_wrist"])
            
            if "state" in o: s = o["state"]
            elif "observation" in o: s = o["observation"]
            else: s = np.zeros(19, dtype=np.float32)
            batch["state"].append(s)

        # ğŸŸ¢ [ç›´æ¥è½¬æ¢] ä¸åšä»»ä½• Resize, Permute æˆ– /255
        # å› ä¸ºç¯å¢ƒå‡ºæ¥çš„å·²ç»æ˜¯ (T, 3, 96, 96) ä¸”æ˜¯ [0, 1] çš„ float äº†
        t_img = torch.from_numpy(np.stack(batch["image"])).float().unsqueeze(0).to(self.device)
        t_wri = torch.from_numpy(np.stack(batch["image_wrist"])).float().unsqueeze(0).to(self.device)
        t_state = torch.from_numpy(np.stack(batch["state"])).float().unsqueeze(0).to(self.device)

        inp = {"image": t_img, "image_wrist": t_wri, "state": t_state}

        try:
            with torch.no_grad():
                result = self.base_policy.predict_action(inp)
        except Exception as e:
            print("\nâŒ Model Inference Failed!")
            # æ‰“å°ä¸€ä¸‹å½¢çŠ¶ä»¥ä¾¿æœ€åç¡®è®¤
            print(f"Input Img Shape: {t_img.shape} (Expected: 1, T, 3, 96, 96)")
            raise e

        all_actions = result["action"][0].cpu().numpy()
        chunk = all_actions[: self.action_chunk_size]
        for act in chunk:
            self.base_action_queue.append(act)

        return self.base_action_queue.popleft()

    def render(self):
        return self.env.render()

    def close(self):
        self.env.close()