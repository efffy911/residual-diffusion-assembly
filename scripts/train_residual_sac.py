import os
import sys
import gymnasium as gym
import numpy as np
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

# ==============================================================================
# ğŸŸ¢ è·¯å¾„è®¾ç½® & å¯¼å…¥ç¯å¢ƒ
# ==============================================================================
current_file_path = os.path.abspath(__file__)
script_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ç¡®ä¿ residual_env.py å°±åœ¨ scripts æ–‡ä»¶å¤¹ä¸‹
try:
    from scripts.residual_env import ResidualPegEnv
except ImportError:
    # å¦‚æœä½ åœ¨æ ¹ç›®å½•è¿è¡Œï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    from residual_env import ResidualPegEnv

def main():
    # --------------------------------------------------------------------------
    # ğŸ”§ [æ ¸å¿ƒè¶…å‚æ•° - Round 6 Rebirth]
    # --------------------------------------------------------------------------
    RUN_NAME = "SAC_Residual_v1"   # ç»™ä»–èµ·ä¸ªå“äº®çš„åå­—
    TOTAL_TIMESTEPS = 500_000                  # å»ºè®®è·‘ä¹…ä¸€ç‚¹ï¼Œåæ­£å¾ˆå¿«
    RESIDUAL_SCALE = 0.01                      # ä¿æŒ 0.01 ä¸å˜
    SEED = 42
    
    # ğŸ‘‡ [è¯·åŠ¡å¿…ä¿®æ”¹] æŒ‡å‘ä½ çš„ Diffusion Policy æƒé‡æ–‡ä»¶
    # è¿™æ˜¯ä½ ä¹‹å‰è®­ç»ƒå¥½çš„é‚£ä¸ª 63% æˆåŠŸç‡çš„æ¨¡å‹
    CKPT_RELATIVE_PATH = "diffusion_policy/data/outputs/2026.01.26/16.00.38_train_diffusion_unet_hybrid_peg_in_hole/checkpoints/base_policy.ckpt"
    BASE_CKPT_PATH = os.path.join(project_root, CKPT_RELATIVE_PATH)
    
    # --------------------------------------------------------------------------
    # 1. åˆ›å»ºç¯å¢ƒ (Factory)
    # --------------------------------------------------------------------------
    if not os.path.exists(BASE_CKPT_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° Base Policy æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„:\n{BASE_CKPT_PATH}")
        return

    def make_env():
        env = ResidualPegEnv(
            base_ckpt_path=BASE_CKPT_PATH,
            residual_scale=RESIDUAL_SCALE,
            residual_clip=0.2,       # è¿™é‡Œçš„ clip å¯¹åº” env é‡Œçš„è®¾ç½®
            action_chunk_size=4,     
            max_steps=200,
            device='cuda:0'          # ç¡®ä¿ base policy åœ¨ GPU ä¸Š
        )
        # Monitor ç”¨äºè®°å½• Reward æ›²çº¿åˆ° Tensorboard
        log_dir = os.path.join(project_root, "data", "logs", RUN_NAME)
        os.makedirs(log_dir, exist_ok=True)
        env = Monitor(env, log_dir)
        return env

    # å‘é‡åŒ–ç¯å¢ƒ
    env = DummyVecEnv([make_env])

    # --------------------------------------------------------------------------
    # 2. å®šä¹‰ SAC æ¨¡å‹
    # --------------------------------------------------------------------------
    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šä» "MultiInputPolicy" æ”¹ä¸º "MlpPolicy"
    # å› ä¸ºç°åœ¨çš„ Observation åªæœ‰ 3 ä¸ªæ•°å­— (x,y,z error)ï¼Œä¸éœ€è¦ CNN å¤„ç†å›¾åƒ
    
    policy_kwargs = dict(
        net_arch=[256, 256],  # ç½‘ç»œä¸ç”¨å¤ªå¤§ï¼Œ256è¶³å¤Ÿäº†
    )

    model = SAC(
        "MlpPolicy",          # ğŸ‘ˆ [é‡ç‚¹] çº¯å‘é‡è¾“å…¥ç”¨ MLP
        env,
        verbose=1,
        seed=SEED,
        
        # --- ä¼˜åŒ–åçš„å‚æ•° ---
        learning_rate=3e-4,   # æ ‡å‡†å­¦ä¹ ç‡å³å¯ï¼Œå› ä¸º Dense Reward å¾ˆå¥½å­¦
        buffer_size=100_000,
        batch_size=256,
        
        # è‡ªåŠ¨è°ƒæ•´ç†µã€‚å› ä¸ºç»´åº¦ä½ (3ç»´)ï¼ŒSAC è‡ªåŠ¨è°ƒå‚ä¼šéå¸¸å‡†ï¼Œä¸ç”¨æˆ‘ä»¬æ“å¿ƒã€‚
        ent_coef='auto',      
        
        gamma=0.99,
        tau=0.005,
        train_freq=1,
        gradient_steps=1,
        policy_kwargs=policy_kwargs,
        tensorboard_log=os.path.join(project_root, "data", "tensorboard"),
        device='cuda:0'
    )

    # --------------------------------------------------------------------------
    # 3. è®¾ç½®å›è°ƒ
    # --------------------------------------------------------------------------
    save_dir = os.path.join(project_root, "data", "models", RUN_NAME)
    
    # æ¯ 5000 æ­¥ä¿å­˜ä¸€æ¬¡ (å› ä¸ºè·‘å¾—å¿«ï¼Œå¯ä»¥å­˜å‹¤å¿«ç‚¹)
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=save_dir,
        name_prefix="rl_model"
    )

    print(f"\nğŸš€ [Round 6] å‡¤å‡°æ¶…æ§ƒ - å¼€å§‹è®­ç»ƒ!")
    print(f"ğŸ‘€ è§‚æµ‹ç©ºé—´: 3ç»´ (Pos Error)")
    print(f"ğŸ¯ å¥–åŠ±æœºåˆ¶: è·ç¦»å¼•å¯¼ (Dense) + æ—¶é—´ç¼©æ”¾æˆåŠŸå¥–åŠ±")
    print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜: {save_dir}")
    print(f"ğŸ“ˆ ç›‘æ§å‘½ä»¤: tensorboard --logdir data/tensorboard\n")

    # --------------------------------------------------------------------------
    # 4. å¼€å§‹ç‚¼ä¸¹
    # --------------------------------------------------------------------------
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=[checkpoint_callback],
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒï¼Œæ­£åœ¨ä¿å­˜æœ€åä¸€æ­¥...")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(save_dir, "rl_model_final"))
    print("âœ… è®­ç»ƒç»“æŸã€‚å» Tensorboard çœ‹çœ‹è¿™ä¸€è½®çš„æ›²çº¿æœ‰å¤šæ¼‚äº®å§ï¼")
    env.close()

if __name__ == "__main__":
    main()